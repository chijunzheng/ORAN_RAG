---
description: 
globs: 
alwaysApply: true
---

# Prompt for Cursor to Implement Contextual Retrieval in O-RAN RAG Pipeline

## Overview

I want to implement the contextual retrieval technique described in Anthropic's recent article to improve my O-RAN RAG pipeline. Contextual retrieval enhances retrieval accuracy by adding document-level context to each chunk before embedding.

## Existing Codebase Structure

My codebase has:
- `src/data_processing/document_chunker.py` - Current chunking implementation
- `src/main.py` - Pipeline orchestration 
- Various other modules for embedding, search, RAG, etc.

## Implementation Requirements

Please implement the following while maintaining code organization and style:

1. Create a new file `src/data_processing/contextual_chunker.py` that extends the existing `DocumentChunker`
2. Update imports in `main.py` to use this new chunker
3. Ensure the implementation preserves all existing functionality

## Technical Details

### 1. Create `contextual_chunker.py`

The new file should:
- Extend the `DocumentChunker` class
- Add Gemini-1.5-flash for context generation
- Override the `assign_ids` and `split_documents` methods
- Add a new `generate_chunk_context` method

Here's the implementation pattern:

```python
import logging
import json
import uuid
from typing import List, Dict
from langchain.schema import Document
from google.cloud import storage
from google.oauth2.credentials import Credentials
from vertexai.generative_models import GenerativeModel, Content, Part, GenerationConfig
from src.data_processing.document_chunker import DocumentChunker, extract_oran_metadata_from_filename

class ContextualChunker(DocumentChunker):
    """
    Extends DocumentChunker to implement contextual retrieval as described in Anthropic's article.
    
    This approach improves retrieval accuracy by adding document-level context to each chunk
    before embedding it. Context is generated using Gemini-1.5-flash to situate the chunk 
    within the entire document.
    """
    
    def __init__(
        self,
        chunk_size: int = 1536,
        chunk_overlap: int = 256,
        separators: List[str] = None,
        gcs_bucket_name: str = None,
        gcs_embeddings_path: str = "embeddings/",
        credentials: Credentials = None,
        min_char_count: int = 100
    ):
        super().__init__(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            separators=separators,
            gcs_bucket_name=gcs_bucket_name,
            gcs_embeddings_path=gcs_embeddings_path,
            credentials=credentials,
            min_char_count=min_char_count
        )
        
        # Initialize Gemini model for context generation
        self.generative_model = GenerativeModel("gemini-1.5-flash-002")
        
        # Define generation config for context generation
        self.context_generation_config = GenerationConfig(
            temperature=0.1,  # Low temperature for more deterministic output
            top_p=0.95,
            max_output_tokens=100  # Limiting to reasonable context size
        )
        
        logging.info("ContextualChunker initialized with Gemini-1.5-flash for context generation.")
    
    def generate_chunk_context(self, document_text: str, chunk_text: str) -> str:
        """
        Generates context for a chunk using Gemini-1.5-flash.
        
        Args:
            document_text (str): The full document text.
            chunk_text (str): The content of the chunk.
            
        Returns:
            str: Generated context that situates the chunk within the document.
        """
        try:
            # Truncate document if it's too large
            max_doc_length = 10000  # Arbitrary limit to avoid context window issues
            if len(document_text) > max_doc_length:
                # Take beginning and end of document
                half_length = max_doc_length // 2
                truncated_doc = document_text[:half_length] + "\n...\n" + document_text[-half_length:]
                document_text = truncated_doc
            
            # Create the prompt as specified in the article
            prompt = f"""
<document>
{document_text}
</document>
Here is the chunk we want to situate within the whole document
<chunk>
{chunk_text}
</chunk>
Please give a short succinct context to situate this chunk within the overall document for the purposes of improving search retrieval of the chunk. Answer only with the succinct context and nothing else.
"""
            
            # Call Gemini model
            content = Content(
                role="user",
                parts=[Part.from_text(prompt)]
            )
            
            response = self.generative_model.generate_content(
                content,
                generation_config=self.context_generation_config
            )
            context = response.text.strip()
            
            # Limit context length for efficiency
            if len(context) > 200:
                context = context[:197] + "..."
            
            logging.debug(f"Generated context: {context}")
            return context
        
        except Exception as e:
            logging.error(f"Failed to generate context: {e}", exc_info=True)
            return "This is part of a document."
    
    def assign_ids(self, split_docs: List[Document]) -> List[Dict]:
        """
        Overrides the parent method to add context generation.
        Assigns unique IDs to chunks and adds context based on the full document.
        
        Args:
            split_docs (List[Document]): List of split Document objects.
            
        Returns:
            List[Dict]: List of chunk dictionaries with IDs and context.
        """
        logging.info("Assigning UUIDs and embedding context into chunks.")
        
        # Group documents by name to reconstruct full documents
        doc_content_map = {}
        for doc in split_docs:
            doc_name = doc.metadata.get('document_name', 'Unknown Document')
            if doc_name not in doc_content_map:
                doc_content_map[doc_name] = {
                    'chunks': [],
                    'content': ""
                }
            doc_content_map[doc_name]['chunks'].append(doc)
            doc_content_map[doc_name]['content'] += doc.page_content + "\n\n"
        
        chunks_with_ids = []
        
        for doc_name, doc_data in doc_content_map.items():
            document_content = doc_data['content']
            
            for doc in doc_data['chunks']:
                char_count = len(doc.page_content)
                if char_count < self.min_char_count:
                    logging.debug(f"Skipped chunk due to low character count: {char_count} < {self.min_char_count}.")
                    continue
                
                chunk_uuid = str(uuid.uuid4())
                document_name = doc.metadata.get('document_name', 'Unknown Document')
                page_number = doc.metadata.get('page_number', 'Unknown Page')
                
                # Extract additional ORAN metadata from the document file name
                extracted_metadata = extract_oran_metadata_from_filename(document_name)
                version = doc.metadata.get('version', extracted_metadata.get('version'))
                workgroup = doc.metadata.get('workgroup', extracted_metadata.get('workgroup'))
                subcategory = doc.metadata.get('subcategory', extracted_metadata.get('subcategory'))
                
                # Generate context for this chunk
                context = self.generate_chunk_context(document_content, doc.page_content)
                
                # Create metadata header
                metadata_header = (
                    f"Document Name: {document_name}; "
                    f"Version: {version}; "
                    f"Workgroup: {workgroup}; "
                    f"Subcategory: {subcategory}; "
                    f"Page: {page_number};"
                )
                
                # Create full content with context
                full_content = f"{metadata_header}\n\nContext: {context}\n\n{doc.page_content}"
                
                chunk_dict = {
                    'id': chunk_uuid,
                    'content': full_content,
                    'document_name': document_name,
                    'page_number': page_number,
                    'char_count': char_count,
                    'metadata': {
                        "document_name": document_name,
                        "version": version,
                        "workgroup": workgroup,
                        "subcategory": subcategory,
                        "page_number": page_number,
                        "context": context
                    }
                }
                chunks_with_ids.append(chunk_dict)
        
        logging.info("Completed assigning UUIDs and embedding context into chunks.")
        return chunks_with_ids
    
    def split_documents(self, documents: List[Document]) -> List[Dict]:
        """
        Splits a list of Document objects into chunks and assigns unique IDs with context.
        This method overrides the parent method but uses its functionality.
        
        Args:
            documents (List[Document]): List of Document objects.
            
        Returns:
            List[Dict]: List of dictionaries containing chunk information with context.
        """
        logging.info("Starting to split documents into chunks with context.")
        split_docs = self.splitter.split_documents(documents)
        logging.info(f"Total chunks created before filtering: {len(split_docs)}")
        
        chunks_with_ids = self.assign_ids(split_docs)
        logging.info(f"Total chunks after filtering and adding context: {len(chunks_with_ids)}")
        return chunks_with_ids
```

### 2. Update `main.py`

In `main.py`, find where `DocumentChunker` is imported and instantiated, and update to:

```python
# Import the new ContextualChunker
from src.data_processing.contextual_chunker import ContextualChunker

# Then, replace the DocumentChunker instantiation with this:
chunker = ContextualChunker(
    chunk_size=chunking_config['chunk_size'],
    chunk_overlap=chunking_config['chunk_overlap'],
    separators=chunking_config['separators'],
    gcs_bucket_name=gcp_config['bucket_name'],
    gcs_embeddings_path=gcp_config['embeddings_path'],
    credentials=auth_manager.credentials,
    min_char_count=chunking_config['min_char_count']
)
```

## Implementation Notes

1. Ensure proper error handling for the Gemini API calls
2. Add appropriate logging at key points
3. Consider performance implications (API rate limits, processing time)
4. Document the contextual retrieval approach in code comments

## Testing Strategy

After implementation, test:
1. The chunking process works correctly with context generation
2. The metadata is properly preserved
3. The chunks are saved in the expected format
4. Retrieval accuracy improves with contextual chunks

Please maintain the existing code organization and style while adding this new functionality.